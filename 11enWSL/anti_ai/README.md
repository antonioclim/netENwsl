# Week 11 anti-AI mechanism

This kit uses a lightweight challenge–evidence–validator scheme intended to make it difficult to submit a fully fabricated solution without actually running the lab.

## Artefacts

For Week 11 the expected artefacts are:

- `artifacts/anti_ai/week11_challenge.json` (issued by staff)
- `pcap/week11_capture.pcap` (captured while you run the required interactions)
- `artifacts/anti_ai/week11_evidence.json` (generated by you)

## What is checked

The validator checks that:

- the challenge is within its TTL
- the challenge token appears in the capture as an `X-AI-Challenge` HTTP header
- at least two distinct backends were observed in the capture (`X-Served-By` or `X-Backend-ID`)
- the capture contains the DNS query name derived from the token (for example `ai-<token>.invalid`)
- the capture file matches the SHA-256 declared in the evidence

## Commands

```bash
# Capture (HTTP + DNS)
./scripts/capture_traffic.py --filter "tcp port 8080 or udp port 53" -o pcap/week11_capture.pcap

# Generate evidence (sends tokenised requests)
python scripts/anti_ai_generate_evidence.py \
  --challenge artifacts/anti_ai/week11_challenge.json \
  --pcap pcap/week11_capture.pcap \
  --lb-url http://localhost:8080/

# Validate
python scripts/anti_ai_validate.py \
  --challenge artifacts/anti_ai/week11_challenge.json \
  --evidence artifacts/anti_ai/week11_evidence.json \
  --pcap pcap/week11_capture.pcap
```

## Signing challenges

If staff set `ANTI_AI_SECRET` in the environment when issuing challenges, the JSON will contain an HMAC signature. The validator can then detect tampering.
